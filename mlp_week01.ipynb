{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8f78c140",
   "metadata": {},
   "source": [
    "# Week 1: Exploratory Data Analysis and Feature Engineering\n",
    "\n",
    "**Student Name 1, Student Name 2**\n",
    "\n",
    "## Aims\n",
    "\n",
    "By the end of this notebook you will \n",
    "\n",
    "* understand and play with the different aspects of data pre-processing\n",
    "* be familiar with tools for exploratory data analysis and visualization\n",
    "* understand the basics of feature engineering\n",
    "* build your first pipeline\n",
    "\n",
    "## Topics and Instructions\n",
    "\n",
    "1. [Problem Definition and Setup](#setup)\n",
    "\n",
    "2. [Exploratory Data Analysis](#eda)\n",
    "\n",
    "3. [Data Preprocessing](#prep)\n",
    "\n",
    "4. [Feature Engineering](#engin)\n",
    "\n",
    "5. [Summary](#sum)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8862515",
   "metadata": {},
   "source": [
    "In lecture this week, we reviewed the general **machine learning pipline**, which following the [\"Machine Learning Project Checklist\"](https://github.com/ageron/handson-ml/blob/master/ml-project-checklist.md) of Geron (2019) can be stuctured as:\n",
    "\n",
    ">- Frame the problem and look at the big picture.\n",
    ">- Get the data.\n",
    ">- Explore the data and gain insights.\n",
    ">- Prepare the data to better expose the underlying data patterns to Machine Learning algorithms.\n",
    ">- Explore many different models and shortlist the best ones.\n",
    ">- Fine-tune your models and combine them into a great solution.\n",
    ">- Present your solution.\n",
    ">- Launch, monitor, and mantain your system.\n",
    "\n",
    "In this week's workshop, we will focus on the initial steps of this pipeline, that is on, data pre-processing, exploratory data analysis and feature engineering.\n",
    "\n",
    "During workshops, you will complete the worksheets together in teams of 2-3, using **pair programming**. During the first few weeks, the worksheets will contain cues to switch roles between driver and navigator. When completing worksheets:\n",
    "\n",
    ">- You will have tasks tagged by (CORE) and (EXTRA). \n",
    ">- Your primary aim is to complete the (CORE) components during the WS session, afterwards you can try to complete the (EXTRA) tasks for your self-learning process. \n",
    ">- Look for the üèÅ as cue to switch roles between driver and navigator.\n",
    ">- In some Exercises, you will see some beneficial hints at the bottom of questions.\n",
    "\n",
    "Instructions for submitting your workshops can be found at the end of worksheet. As a reminder, you must submit a pdf of your notebook on Learn by 16:00 PM on the Friday of the week the workshop was given. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23c0c3c9",
   "metadata": {},
   "source": [
    "# Problem Definition and Setup <a id='setup'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73d830b3",
   "metadata": {},
   "source": [
    "## Packages\n",
    "\n",
    "Now lets load in some packages to get us started. The following are widely used libraries to start working with Python in general."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5fba9ca3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy import stats"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71cd4cc0",
   "metadata": {},
   "source": [
    "If you need to install any packages from scratch, you need to install the related library before calling it. For instance, [feature-engine](https://feature-engine.trainindata.com/en/latest/) is a Python library for Feature Engineering and Selection, which: \n",
    "\n",
    "- contains multiple transformers to engineer and select features to use in machine learning models.\n",
    "\n",
    "- preserves scikit-learn functionality with methods fit() and transform() to learn parameters from and then transform the data (we will learn more about these throughout the course!)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9fb8d2d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# To install the feature-engine library (if not already installed)\n",
    "!pip install feature-engine"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0ca0145",
   "metadata": {},
   "source": [
    "In some cases, we may need only a component of the whole library. If this is the case, it is possible to import specific things from a module (library), using the following line of code:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39ef6aac",
   "metadata": {},
   "outputs": [],
   "source": [
    "from feature_engine.imputation import EndTailImputer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4400191a",
   "metadata": {},
   "source": [
    "## Problem\n",
    "\n",
    "Now, it is time move on to the next step.\n",
    "\n",
    "> You are asked to build a model of housing prices in California using the California census data. This data has metrics such as the population, median income, median housing price, and so on for each block group in California. Block groups are the smallest geographical unit for which the US Census Bureau publishes sample data (a block group typically has a population of 600 to 3,000 people). We will just call them ‚Äúdistricts‚Äù for short.\n",
    ">\n",
    "> **Your model should learn from this data and be able to predict the median housing price in any district, given all the other metrics.**\n",
    ">\n",
    "> The first question to ask your boss is what exactly is the business objective; building a model is probably not the end goal. **How does the company expect to use and benefit from this model?** This is important because it will determine how you frame the problem, what algorithms you will select, what performance measure you will use to evaluate your model, and how much effort you should spend tweaking it.\n",
    ">\n",
    "> The next question to ask is **what the current solution looks like (if any)**. It will often give you a reference performance, as well as insights on how to solve the problem. Your boss answers that the district housing prices are currently estimated manually by experts: a team gathers up-to-date information about a district, and when they cannot get the median housing price, they estimate it using complex rules.\n",
    ">\n",
    "> This is costly and time-consuming, and their estimates are not great; in cases where they manage to find out the actual median housing price, they often realize that their estimates were off by more than 20%. This is why the company thinks that it would be useful to train a model to predict a district‚Äôs median housing price given other data about that district. The census data looks like a great dataset to exploit for this purpose, since it includes the median housing prices of thousands of districts, as well as other data.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0831296-b285-4fcd-a4bd-c650afc17e80",
   "metadata": {},
   "source": [
    "### üö© Exercise 1 (CORE)\n",
    "\n",
    "Using the information above answer the following questions about how you may design your machine learning system.\n",
    "\n",
    "a) Is this a supervised or unsupervised learning task? "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0969486-871f-451e-95ac-5fda59b50e75",
   "metadata": {},
   "source": [
    "_Type your answer here!_"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2a490f1-f01c-45ad-b5db-2012f5d90b5f",
   "metadata": {},
   "source": [
    "b) Is this a classification, regression, or some other task?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c293a886-4353-479f-a1b6-b75b42c087ac",
   "metadata": {},
   "source": [
    "_Type your answer here!_"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "980d7283-8c7b-4f97-89fc-3585b51fa53a",
   "metadata": {},
   "source": [
    "c) Suppose you are only required to predict if a district's median housing prices are \"cheap,\" \"medium,\" or \"expensive\". Will this be the same or a different task?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "840905cc-cb04-4b48-9663-0428edf03a86",
   "metadata": {},
   "source": [
    "_Type your answer here!_"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac2d6efb",
   "metadata": {},
   "source": [
    "## Data Download\n",
    "\n",
    "The data we will be using this week is a modified version of the California Housing dataset. We can get the data a number of ways. The easiest is just to load it from the working directory that we are working on (where we have already downloaded it to)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a90236e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "housing = pd.read_csv(\"housing.csv\")\n",
    "housing.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a744a3c",
   "metadata": {},
   "source": [
    "# Exploratory Data Analysis <a id='eda'></a>\n",
    "\n",
    "In this section we are going to start with exploring the California Housing data using methods that you will likely already be familiar with.\n",
    "\n",
    "Data can come in a broad range of forms encompassing a collection of discrete objects, numbers, words, events, facts, measurements, observations, or even descriptions of things. Processing data using exploratory data analysis (EDA) can elicit useful information and knowledge by examining the available dataset to discover patterns, spot anomalies, test hypotheses, and check assumptions. \n",
    "\n",
    "Let's start by examining the [Data Dictionary](https://www.kaggle.com/camnugent/california-housing-prices) and the variables available:\n",
    "\n",
    "> `longitude`: A measure of how far west a house is; a higher value is farther west\n",
    ">\n",
    "> `latitude`: A measure of how far north a house is; a higher value is farther north\n",
    ">\n",
    "> `housingMedianAge`: Median age of a house within a block; a lower number is a newer building\n",
    ">\n",
    "> `totalRooms`: Total number of rooms within a block\n",
    ">\n",
    "> `totalBedrooms`: Total number of bedrooms within a block\n",
    ">\n",
    "> `population`: Total number of people residing within a block\n",
    ">\n",
    "> `households`: Total number of households, a group of people residing within a home unit, for a block\n",
    ">\n",
    "> `medianIncome`: Median income for households within a block of houses (measured in tens of thousands of US Dollars)\n",
    ">\n",
    "> `medianHouseValue`: Median house value for households within a block (measured in US Dollars)\n",
    ">\n",
    "> `oceanProximity`: Location of the house w.r.t ocean/sea"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ffa49b60",
   "metadata": {},
   "source": [
    "### üö© Exercise 2 (CORE)\n",
    "\n",
    "a) Examine the datatypes for each column calling [`info()`](https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.info.html). What is the total number of observations and total number of variables? What is the type of each variable?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad2a799c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Code for your answer here!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0673d936",
   "metadata": {},
   "source": [
    "_Type your answer here!_"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8379e17f",
   "metadata": {},
   "source": [
    "b) From the information provided above, can you anticipate any data cleaning we may need to do?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64363932",
   "metadata": {},
   "source": [
    "_Type your answer here!_"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7857d21",
   "metadata": {},
   "source": [
    "### üö© Exercise 3  (CORE)\n",
    "\n",
    "a) Use descriptive statistics and histograms to examine the distributions of the numerical attributes.\n",
    "<br><br>\n",
    "<details><summary><b><u>Hint</b></u></summary>\n",
    "    \n",
    "- <code>.describe()</code> can be used to create summary descriptive statistics on a pandas dataframe.\n",
    "- You can use a [<code>sns.histplot</code>](https://seaborn.pydata.org/generated/seaborn.histplot.html) to create histograms\n",
    "    \n",
    "</details>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3cffb13b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Code for your answer here!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7393857",
   "metadata": {},
   "source": [
    "b) Can you identify other pre-processing/feature engineering steps we may need to do? Which variables represent counts and how are they distributed?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "572c9011",
   "metadata": {},
   "source": [
    "_Type your answer here!_"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2d38752",
   "metadata": {},
   "source": [
    "c) One thing you may have noticed from the histogram is that the median income, housing median age, and the median house value are capped. The median house value capping (this being our target value) may or may not be a problem depending on your client. If we needed precise predictions beyond $\\$500,000$, we may need to either collect proper labels/outputs for the districts whose labels were capped or remove these districts from the data. Following the latter, remove all districts whose median house value is capped. How many observations are there now?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47610e1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Code for your answer here!\n",
    "# Remove the cases where median_house_value >= 500,000$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4aba6db",
   "metadata": {},
   "source": [
    "\n",
    "### üö© Exercise 4 (CORE)\n",
    "\n",
    "What are the possible categories for the `ocean_proximity` variable? Are the number of instances in each category similar? \n",
    "<br><br>\n",
    "<details><summary><b><u>Hint</b></u></summary>\n",
    "    \n",
    "- [<code>value_counts()</code>](https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.Series.value_counts.html) can be used to count the values of the categories a pandas series.\n",
    "- You can use a [<code>sns.countplot</code>](https://seaborn.pydata.org/generated/seaborn.countplot.html) to create barplot with the number of instances of each category \n",
    "    \n",
    "</details>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81f27705",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Code for your answer here!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "466f4d66",
   "metadata": {},
   "source": [
    "_Type your answer here!_"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8db2956a",
   "metadata": {},
   "source": [
    "üèÅ **Now, is a good point to switch driver and navigator**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fabcef64",
   "metadata": {},
   "source": [
    "### üö© Exercise 5 (CORE)\n",
    "\n",
    "Examine if/which of the features are correlated to each other. Are any of the features correlated with our output (`median_house_value`) variable?\n",
    "\n",
    "- Can you think of any reason why certain features may be correlated?\n",
    "\n",
    "- How might we use this information in later steps of our model pipeline?\n",
    "\n",
    "<br><br>\n",
    "<details><summary><b><u>Hint</b></u></summary>\n",
    "    \n",
    "- <code>.corr()</code> can be used to compute the correlations.\n",
    "- You can use a [<code>sns.heatmap</code>](https://seaborn.pydata.org/generated/seaborn.heatmap.html) to visualize the correlations\n",
    "    \n",
    "</details>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5802979",
   "metadata": {},
   "outputs": [],
   "source": [
    "#¬†Code for your answer here!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99d5c518",
   "metadata": {},
   "source": [
    "_Type your answer here!_"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d96a23f",
   "metadata": {},
   "source": [
    "### üö© Exercise 6 (CORE)\n",
    "\n",
    "Use `sns.pairplot` to further investigate the joint relationship between each pair of variables. What insights into the data might this provide over looking only at the correlation?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f4ea6a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Code for your answer here!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1b83b06",
   "metadata": {},
   "source": [
    "_Type your answer here!_"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99f2e060",
   "metadata": {},
   "source": [
    "# Data Pre-Processing <a id='prep'></a>\n",
    "\n",
    "Now we have some familiarity with the data though EDA, lets start preparing our data to be modelled.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "648d0b12",
   "metadata": {},
   "source": [
    "## Data Cleaning\n",
    "\n",
    "Let's start with some basic data cleaning steps. For example, we may want to:\n",
    "- deal with duplicated, inconsitencies or typos in the data, \n",
    "- handle missing data,\n",
    "- remove uninformative features (e.g. subject identifiers), \n",
    "- fix variable types, \n",
    "- adjust data codes (e.g. missing variables may be coded as ‚Äò999‚Äô instead NA),\n",
    "- optionally remove outliers. \n",
    "\n",
    "Let's start with the former.\n",
    "\n",
    "### Data Duplication and Errors\n",
    "We want to remove duplicates, that may have accidently been entered in the database twice, as they may bias our fitted model. In other words, we may potentially *overfit* to this subset of points. However, care should usually be taken to check they are not _real_ data with identical values.\n",
    "\n",
    "There a number of ways we could identify duplicates, the simplist (and the approach we'll focus on) is just to find observations with the same feature values. Of course this will not identify things such as spelling errors, missing values, address changes, use of aliases, etc. This may commonly happen with categorical or text data, and checking the unique values is recommended. In general for such errors, more complicated methods along with manual assessment may be needed. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81217fbc",
   "metadata": {},
   "source": [
    "### üö© Exercise 7 (CORE)\n",
    "\n",
    "a) Are there any duplicated values in the data? If so how many?\n",
    "<br>\n",
    "<details><summary><b><u>Hint</b></u></summary>\n",
    "With Pandas dataframes you can use `.duplicated()` to get a boolean of whether something is a duplicate and then use `.sum()` to count how many there are.\n",
    "</details>\n",
    "\n",
    "\n",
    "b) What are the unique values of the categorical variable? Are there any duplicated categories arising from misspellings?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63be94a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Code for your answer here!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64c7d9e2",
   "metadata": {},
   "source": [
    "_Type your answer here!_"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d0440d0",
   "metadata": {},
   "source": [
    "### Outlier Detection\n",
    "\n",
    "An **Outlier** is a data point that lies abnormally far from other observations and may distort the model fit and results. In general, it is a good idea to examine if any outliers are present during preprocessing.  In some cases, you may want to drop these observations or cap their values (see https://feature-engine.trainindata.com/en/1.8.x/api_doc/outliers/index.html). However this may not be appropriate without explicit knowledge and testing if they are really outliers or not. In particular, when you drop or cap those observations you can discard important information unwittingly!\n",
    "\n",
    "We will use basic statistics in order to try to identify outliers. A simple method of detecting outliers is to use the **inter-quartile range (IQR) proximity rule** (Tukey fences) which states that a value is an outlier if it falls outside these boundaries:\n",
    "\n",
    "- Upper boundary = 75th quantile + (IQR * $k$) \n",
    "\n",
    "- Lower boundary = 25th quantile - (IQR * $k$)\n",
    "\n",
    "where IQR = 75th quantile - 25th quantile (the length of the box in the boxplot). This is used to construct the whiskers in the boxplot, where $k$ is a nonnegative constant which is typically set to 1.5 (the default value in [`sns.boxplot`](https://seaborn.pydata.org/generated/seaborn.boxplot.html)). However, it is also common practice to find extreme values by setting $k$ to 3."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8e01d4e",
   "metadata": {},
   "source": [
    "### üö© Exercise 8 (EXTRA)\n",
    "\n",
    "a) Can you identify any potential outliers using the generated boxplots below? Do you think any points should be removed?\n",
    "\n",
    "b) Try changing $k$, defining the length of the whiskers, to 3 in `sns.boxplot`. Can you still identify any potential outliers?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "116d9be4",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(figsize = (15,10), ncols = (housing.shape[1]-1)//3, nrows = 3, sharex = True)\n",
    "axes = axes.flatten()\n",
    "\n",
    "for i, ax in enumerate(axes):\n",
    "    sns.boxplot(y = housing.iloc[:,i], ax = ax) \n",
    "    ax.set_title(housing.iloc[:,i].name)\n",
    "    ax.set_ylabel(\"\")\n",
    "    \n",
    "plt.suptitle(\"Boxplots\")\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1461ae4",
   "metadata": {},
   "source": [
    "_Type your answer here!_"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "beefda73",
   "metadata": {},
   "source": [
    "üèÅ **Now, is a good point to switch driver and navigator**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce7a1f5d",
   "metadata": {},
   "source": [
    "### Missing Data\n",
    "\n",
    "Most ML models cannot handle missing values, and as we saw earlier, there are some present in `total_bedrooms`. We also saw that values of `median_house_value` are capped at $\\$500,000$. This is another form of missingness, which is **informative** for missing values (i.e. the missing values are greater than $\\$500,000$). However, we will focus on methods for dealing with missingness in our features and not the target variable. \n",
    "\n",
    "As such, let's start by splitting our **features** from our **target** variable in the data set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8bc8a343",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extracting the features from the data\n",
    "X = housing.drop(\"median_house_value\", axis = 1)\n",
    "features = list(X.columns)\n",
    "print(features)\n",
    "print(X.shape)\n",
    "display(X.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9eeebd00",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extracting the target features from the data\n",
    "y = housing[\"median_house_value\"].copy()\n",
    "print(y.shape)\n",
    "display(y.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ae86bdf",
   "metadata": {},
   "source": [
    "\n",
    "There are a number of ways we can deal with missing values. The simplest is to just **remove NA values**. \n",
    "We can do this in two ways by either:\n",
    "\n",
    "1. Getting rid of the corresponding observations (deleting the corresponding rows).\n",
    "2. Getting rid of the whole attribute (deleting the corresponding columns).\n",
    "\n",
    "To is relatively straight forward by running `housing.dropna()` with either the `axis` set to `0` or `1` (depending if we want to remove rows or columns) before splitting our data into features (`X`) and outputs (`y`). \n",
    "\n",
    "### üö© Exercise 9  (CORE)\n",
    "\n",
    "Use `dropna()` to remove the missing observations. What is the shape of the feature matrix after dropping the missing observations?\n",
    "\n",
    "__Notes__\n",
    "\n",
    "- It may be tempting to overwrite `X` while working on our pre-processing steps. __Don't do this!__ We will run these objects through our pipeline which combines missing data steps with other steps later, so if you want to test your function make sure to assign the output to tempory objects (e.g. `X_`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e73547f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Code for your answer here!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97cda5b1",
   "metadata": {},
   "source": [
    "Instead of simply dropping missing data, we may instead want to use other **imputation** methods. From here on in, we will be creating functions for our data transformations. Later, we will see why this is really useful to define our **model pipeline**, which allows us to chain together transformations and steps in a reproducible way. \n",
    "\n",
    "In this course we are mostly going to be using `Scikit-learn`, with a little `Keras` at the end for neural networks. Scikit-learn is an open source machine learning library that supports supervised and unsupervised learning (https://scikit-learn.org/stable/getting_started.html). It also provides various tools for model fitting, data preprocessing, model selection and evaluation, and many other utilities. \n",
    "\n",
    "We will first focus on the _transformer_ class within Scikit-learn, which provides functions for missing data imputation along with many others useful for data pre-processing and feature engineering.\n",
    "\n",
    "#### **Transfomers**\n",
    "\n",
    "If we want to __alter the features__ of our data, we need a _transfomer_.\n",
    "\n",
    "- Transformers are classes that follow the scikit-learn API in Scikit-Learn [clean](https://scikit-learn.org/stable/modules/preprocessing.html#preprocessing), [impute](https://scikit-learn.org/1.5/modules/impute.html), [reduce](https://scikit-learn.org/stable/modules/unsupervised_reduction.html#data-reduction), [expand](https://scikit-learn.org/stable/modules/kernel_approximation.html#kernel-approximation), or [generate](https://scikit-learn.org/stable/modules/feature_extraction.html#feature-extraction) feature representations.\n",
    "\n",
    "- Transformers are classes with a `.fit()` method, which learn model parameters (e.g. mean for mean imputation) from a training set, and a `.transform()` method which applies this transformation model to data. To create a custom transformer, all you need is to create a class that implements three methods: `fit()`, `transform()`, and `fit_transform()`.\n",
    "\n",
    "Therefore to transform a dataset, each sampler implements:\n",
    "\n",
    "```\n",
    "obj.fit(data)\n",
    "data_transformed = obj.transform(data)\n",
    "```\n",
    "\n",
    "or simply...\n",
    "\n",
    "```\n",
    "data_transformed = obj.fit_transform(data)`\n",
    "```\n",
    "\n",
    "See more details: https://scikit-learn.org/stable/data_transforms.html. In the following subsections, we will see examples of _transformers_ for categorical and numerical variables."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "144a1d38",
   "metadata": {},
   "source": [
    "#### Data Imputation\n",
    "\n",
    "Instead of removing the missing data we can set it to some value. To do this, Scikit-Learn provides various transformers, including:\n",
    "- [`SimpleImputer`](https://scikit-learn.org/1.5/modules/generated/sklearn.impute.SimpleImputer.html) which provides simple strategies (e.g. `\"mean\"`, `\"median\"` for numerical features and `\"most_frequent\"` for categorical features). \n",
    "- You can also add a missing indicator with the option `add_indicator=True` in `SimplerImputer`, or use the transfomer [`MissingIndicator'](https://scikit-learn.org/stable/modules/generated/sklearn.impute.MissingIndicator.html). This may be useful in the case when missing features may be provide information for predicting the target (e.g. obese patients may prefer not to report bmi, thus, this missingness could be useful for estimating the risk of health conditions or diseases). \n",
    " - Beyond simple imputation strategies, sklearn also provides more advanced imputation strategies in [`IterativeImputer`](https://scikit-learn.org/1.5/modules/generated/sklearn.impute.IterativeImputer.html#sklearn.impute.IterativeImputer) and [`KNNImputer`](https://scikit-learn.org/1.5/modules/generated/sklearn.impute.KNNImputer.html)\n",
    " - Other strategies are also available in [`feature_engine.imputation`](https://feature-engine.trainindata.com/en/1.8.x/user_guide/imputation/index.html). Such as [`EndTailImputer`](https://feature-engine.trainindata.com/en/1.8.x/api_doc/imputation/EndTailImputer.html), which is useful when missing values are located in the tails (e.g. capped values for privacy)\n",
    "\n",
    "Let's start with the `SimpleImputer` to learn about transfomers and how to deal with missing data in sklearn."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7cab651e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.impute import SimpleImputer\n",
    "\n",
    "# First create the imputer object/transformer\n",
    "num_imputer = SimpleImputer(strategy=\"median\")\n",
    "\n",
    "# Now fit the object to the data\n",
    "num_imputer.fit(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d55245b",
   "metadata": {},
   "source": [
    "Unfortunately, when we applied this to our data, we get the following error:\n",
    "\n",
    "```\n",
    "ValueError: Cannot use median strategy with non-numeric data:\n",
    "could not convert string to float:\n",
    "```\n",
    "\n",
    "This is because the \"median\" strategy can only be used with numerical attributes so we need a way of only applying imputation to certain attributes. We could temporarily remove the categorical feature from our data to apply our function, or apply the function to a subset of the data and assign the output to the same subset.\n",
    "\n",
    "However scikit-learn has a handy function to specify what column we want to apply a function to!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "883a5865",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.compose import ColumnTransformer\n",
    "\n",
    "# Names of numerical columns\n",
    "numcols = features[:-1]\n",
    "print(numcols)\n",
    "catcols = [features[-1]]\n",
    "print(catcols)\n",
    "\n",
    "num_cols_imputer = ColumnTransformer(\n",
    "    # apply the `num_imputer` to all columns apart from the last\n",
    "    [(\"num\", num_imputer, numcols)],\n",
    "    # don't touch all other columns, instead concatenate it on the end of the\n",
    "    # changed data.\n",
    "    remainder = \"passthrough\"\n",
    ") \n",
    "\n",
    "num_cols_imputer.fit(X)\n",
    "\n",
    "# Print the median values computed by calling fit\n",
    "print(\"Computed median values for each numerical feature:\")\n",
    "print(num_cols_imputer[\"num\"].statistics_)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4cb1b205",
   "metadata": {},
   "source": [
    "After using `.fit`, our object now has a number of attributes, including `statistics_` which stores the median value for each numerical attribute on the training set. This value can be used when validating and testing the model as it will be used if there is missing data in the new data.\n",
    "\n",
    "__Note__\n",
    "- The fitted `ColumnTransformer` contains a list of transformers, stored in the attribute `transformers_`. We named the first transformer in the list `num`. Try running `num_cols_imputer.transformers_` to see the names and types of the transformers in the list. \n",
    "- To access the fitted `num_imputer` in this case,  `num_cols_imputer[\"num\"]` is a shortcut to access the named transformer in the list. \n",
    "\n",
    "Now, let's call `transform` to our fitted objected to impute the missing values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1de0122",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_ = num_cols_imputer.transform(X)\n",
    "print(\"Number of Missing Values\")\n",
    "pd.DataFrame(X_, columns = features).isna().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61027a2e",
   "metadata": {},
   "source": [
    "### üö© Exercise 10  (CORE)\n",
    "\n",
    "In addition to median imputation, alter your transformer to also include a missing indicator. What is the shape of the transformed feature matrix? Use the method `.get_feature_names_out()` to print the names of the new features.\n",
    "\n",
    "**Note:** You may want to add the option `verbose_feature_names_out = False` in your `ColumnTransformer` to reduce the length of the feature names."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b817c870",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Code for your answer here!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b6726df",
   "metadata": {},
   "source": [
    "üèÅ **Now, is a good point to switch driver and navigator**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9872550",
   "metadata": {},
   "source": [
    "# Feature Engineering <a id='engin'></a>\n",
    "\n",
    "As discussed in the lectures, feature engineering is where we extract features from data and transform them into formats that are suitable for machine learning models. Today, we will have a look at two main cases that are present in our data: **categorical** and **numerical** values.\n",
    "\n",
    "Feature engineering also requires a _transformer_ class to __alter the features__."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db944478",
   "metadata": {},
   "source": [
    "## Categorical Variables\n",
    "\n",
    "- In the dataset, we have an text attribute (`ocean_proximity`) that we already had to treat differently when cleaning and visualizing the data. This extends to feature engineering as well, where we need to use separate methods than those used with numerical variables.\n",
    "\n",
    "- If we look at the unique values of this attribute, we will see that there are a limited number of possible values which represent a category. We need a way of encoding this information into our modeling framework **by converting our string/categorical variable into a numeric representation** that can be included in our models.\n",
    "\n",
    "If we have a binary categorical variable (two levels) we could do this by picking one of the categorical levels and encode it as 1 and the other level as 0. \n",
    "\n",
    "However, in this case as we have multiple categories, we would probably want to use another encoding method. To illustrate, we can try encoding the the categorical feature `ocean_proximity` using both the `OrdinalEncoder` and `OneHotEncoder` available in `sklearn.preprocessing`.\n",
    "\n",
    "__Side Notes__\n",
    "\n",
    "- The output of the `OneHotEncoder` provided in Scikit-Learn is a SciPy _sparse matrix_, instead of a NumPy array. These are useful when you have lots of categories as your matrix becomes mostly full of 0's. To store all these 0's takes up unneccesary memory, so instead a sparse matrix just stores the location of nonzero elements. The good news is that you can use a sparse matrix similar to a numpy matrix, but if you wanted to, you can convert it to a dense numpy matrix using `.toarray()`.\n",
    "\n",
    "- The above does not seem to be the case if passed through a `ColumnTransformer`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57fd696b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import OrdinalEncoder\n",
    "# Defining the OrdinalEncoder\n",
    "ordinal_encoder = OrdinalEncoder()\n",
    "\n",
    "encoder = ColumnTransformer(\n",
    "    # apply the ordinal_encoder to the last column\n",
    "    [(\"cat\", ordinal_encoder, catcols)],\n",
    "    remainder=\"passthrough\",\n",
    "    verbose_feature_names_out=False)\n",
    "\n",
    "# fitting the encoder defined above\n",
    "X_ = encoder.fit_transform(X) \n",
    "\n",
    "# Accessing the fitted ordinal encoder (encoder[\"cat\"]) to see how the categories were mapped\n",
    "display(dict(zip(list(encoder[\"cat\"].categories_[0]), range(5))))\n",
    "\n",
    "# Display the first few rows of the transformed data\n",
    "display(pd.DataFrame(X_, columns = encoder.get_feature_names_out()).head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a574797",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import OneHotEncoder\n",
    "# Defining the OneHotEncoder\n",
    "onehot_encoder = OneHotEncoder()\n",
    "\n",
    "encoder = ColumnTransformer(\n",
    "    # apply the onehot_encoder to the last column\n",
    "    [(\"cat\", onehot_encoder, catcols)],\n",
    "    remainder=\"passthrough\",\n",
    "    verbose_feature_names_out=False) \n",
    "\n",
    "X_ = encoder.fit_transform(X) \n",
    "\n",
    "# Display the first few rows of the transformed data\n",
    "display(pd.DataFrame(X_, columns = encoder.get_feature_names_out()).head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "477773a9",
   "metadata": {},
   "source": [
    "### üö© Exercise 11 (CORE)\n",
    "\n",
    "a) What is the main difference between two methods regarding the obtained features? Which encoding method do you think is most appropriate for this variable and why?\n",
    "\n",
    "b) How sensible is the default ordering of the ordinal encoder? Use the parameter `categories` of `OrdinalEncoder` to apply a different ordering."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff02c9e6",
   "metadata": {},
   "source": [
    "_Type your answer here!_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da3ac8b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Code for your answer here!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39bed85a",
   "metadata": {},
   "source": [
    "### üö© Exercise 12 (EXTRA)\n",
    "\n",
    "Another handy feature of `OneHotEncoder` and `OrdinalEncoder` is that infrequent categories can be aggregated into a single feature/value. The parameters to enable the gathering of infrequent categories are `min_frequency` and `max_categories`.\n",
    "\n",
    "Use the `max_categories` attribute to set the maximum number of categories to 4. Use the `get_feature_names_out()` method of `OneHotEncoder` to print the new category names. Which two features have been combined?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a9b4f3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Code for your answer here!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ddcf0b8",
   "metadata": {},
   "source": [
    "_Type your answer here!_"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8d36555",
   "metadata": {},
   "source": [
    "### üö© Exercise 13 (EXTRA)\n",
    "\n",
    "When there are many unordered categories, another useful encoding scheme is [`TargetEncoder`](https://scikit-learn.org/1.5/modules/generated/sklearn.preprocessing.TargetEncoder.html#sklearn.preprocessing.TargetEncoder) which uses the target mean conditioned on the categorical feature for encoding unordered categories. Whereas one-hot encoding would greatly inflate the feature space if there are a very large number of categories (e.g. zip code or region), `TargetEncoder` is more parsimonious. \n",
    "\n",
    "Use target encoding of ocean proximity. What are the numerical values assigned to the categories?\n",
    "\n",
    "**Caution:** when using this transformer, be careful to avoid data leakage and overfitting by integrating it properly in your model pipeline! We will learn more about this later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41a4a6e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Code for your answer here!\n",
    "\n",
    "# Create a target encoder\n",
    "from sklearn.preprocessing import TargetEncoder"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca69368c",
   "metadata": {},
   "source": [
    "## Numerical Variables"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc2164de",
   "metadata": {},
   "source": [
    "### Feature Scaling\n",
    "\n",
    "As we will discuss in later weeks, many machine learning algorithms are sensitive to the scale and magnitude of the features, and especially differences in scales across features. For these algorithms, feature scaling will improve performance. \n",
    "\n",
    "For example, let's investigate the range of the features in our dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48606ca3",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(15,5))\n",
    "\n",
    "plt.boxplot(X[numcols], vert = False) \n",
    "plt.xscale(\"symlog\") \n",
    "plt.ylabel(\"Feature\") \n",
    "plt.xlabel(\"Feature Range\")\n",
    "\n",
    "ax.set_yticklabels(numcols)\n",
    "\n",
    "plt.suptitle(\"Feature Range for the Training Set\")\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2eb89ff1",
   "metadata": {},
   "source": [
    "There are various options in scikit learn for feature scaling, including: \n",
    "\n",
    "- Standardization (`preprocessing.StandardScaler`)\n",
    "\n",
    "- Min-Max Scaling (`preprocessing.MinMaxScaler`)\n",
    "\n",
    "- l2 Normalization (`preprocessing.normalize`)\n",
    "\n",
    "- RobustScaler(`preprocessing.RobustScaler`)\n",
    "\n",
    "- Scale with maximum absolute value (`preprocessing.MaxAbsScaler`)\n",
    "\n",
    ">- As scaling generally improves the performance of most models when features cover a range of scales, it is probably a good idea to apply some sort of scaling to our data before fitting a model. \n",
    ">- *Standardization* (or *variance scaling*), is the most common, but there are a number of other types, as listed above."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb8cf3cb",
   "metadata": {},
   "source": [
    "### üö© Exercise 14 (CORE)\n",
    "\n",
    "Try implementing at least two different scalers for the `total_rooms` and `total_bedrooms` variables. Make a scatter plot of the original and transformed features to see the main differences. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bcc6a3b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Code for your answer here!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9dfbd251",
   "metadata": {},
   "source": [
    "\n",
    "### Power Transformation\n",
    "\n",
    "In some cases, we may wish to apply transformations to our data, so that they have a more Gaussian distribution. For example, log transformations are useful for altering count data to have a more normal distribution as they pull in the more extreme high values relative to the median, while stretching back extreme low values away from the median. You can use a log transformation with either the pre-made `LogTransformer()` from `feature_engine.transformation`, or a custom function and `sklearn.preprocessing.FunctionTransformer`.\n",
    "\n",
    "More generally, the natural logarithm, square root, and inverse transformations are special cases of the **Box-Cox** family of transformations (Box and Cox 1964). The question is **why do we need such a transformation and when?** \n",
    "\n",
    "- Note that, the method is typically used to transform the outcome, but can also be used to transform predictors. \n",
    "\n",
    "- The method assumes that the variable takes only positive values. If there are any zero or negative values, we can 1) shift the distribution towards positive values by adding a constant, or 2) use the **Yeo-Johnson transformation** (Yeo and Johnson 2000).\n",
    "\n",
    "- In general, transormations can make interpretations more difficult, thus **you should think carefully if they are needed**, particularly if they only result in modest improvements in model performance. Moreover, finding a suitable transformation is typically a trial-and-error process.  \n",
    "\n",
    "- Moreover, if you are transforming the features, you should also consider how this alters the relationship with the target variable. \n",
    "\n",
    "The Yeo-Johnson transformation is defined as:\n",
    "\n",
    "$\\tilde{y} = \\left\\{ \\begin{array}{l l} \\frac{(y+1)^{\\lambda} - 1}{\\lambda}, & \\lambda \\neq 0 \\text{ and } y \\geq 0  \\\\ \\log (y +1), & \\lambda = 0 \\text{ and } y \\geq 0 \\\\   -\\frac{(1-y)^{2-\\lambda} - 1}{2-\\lambda}, & \\lambda \\neq 2 \\text{ and } y < 0 \\\\ -\\log (1-y), & \\lambda = 2 \\text{ and } y < 0\\end{array} \\right.,$\n",
    "\n",
    "with the Box-Cox transformation as a special case (applied to $y-1$). \n",
    "\n",
    "Because the parameter of interest is in the exponent, this type of transformation is called a **power transformation** and is implemented in sklearn's [`PowerTransformer`](https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.PowerTransformer.html). The parameter $\\lambda$ is estimated from the data, and some values of $\\lambda$ relate to common transformations, such as (for $y \\geq 0$):\n",
    "\n",
    ">* $\\lambda = 1$ (no transformation)   \n",
    ">* $\\lambda = 0$ (log)   \n",
    ">* $\\lambda = 0.5$ (square root)   \n",
    ">* $\\lambda = -1$ (inverse)\n",
    "\n",
    "- Using the code below, if `lmbda=None` then the function will \"find the lambda that maximizes the log-likelihood function and return it as the second output argument\"\n",
    "\n",
    "- Notice that we can not use `lambda` directly since it conflicts with the available object called `lambda`, this is the reason we preferred the indicator name as `lmbda`\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd0d1eb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(figsize = (15,5), ncols = 4, nrows=2, sharey = True)\n",
    "axes = axes.flatten()\n",
    "sns.histplot(data = X['households'], ax = axes[0])\n",
    "axes[0].set_title(\"Raw Counts\")\n",
    "\n",
    "for i, lmbda in enumerate([0, 0.25, 0.5, 0.75, 1., 1.25, 1.5]):\n",
    "    \n",
    "    house_box_ = stats.boxcox(X['households'].astype(float), lmbda = lmbda)\n",
    "    sns.histplot(data = house_box_, ax = axes[i + 1])\n",
    "    axes[i + 1].set_title(r\"$\\lambda$ = {}\".format(lmbda))\n",
    "    \n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "fig, axes = plt.subplots(figsize = (15,5), ncols = 4, nrows=2, sharey = True)\n",
    "axes = axes.flatten()\n",
    "sns.scatterplot(x = X['households'], y = y, ax = axes[0])\n",
    "axes[0].set_title(\"Raw Counts\")\n",
    "\n",
    "for i, lmbda in enumerate([0, 0.25, 0.5, 0.75, 1., 1.25, 1.5]):\n",
    "    \n",
    "    house_box_ = stats.boxcox(X['households'].astype(float), lmbda = lmbda)\n",
    "    sns.scatterplot(x = house_box_, y = y, ax = axes[i + 1])\n",
    "    axes[i + 1].set_title(r\"$\\lambda$ = {}\".format(lmbda))\n",
    "   \n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33c5645d",
   "metadata": {},
   "source": [
    "We can find the $\\lambda$ that maximizes the log-likelihood function using scipy's `boxcox` function or sklearn's `PowerTransformer`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abbc7289",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find the MLE for lambda (using scipy's boxcox function)\n",
    "house_box_, bc_params = stats.boxcox(X['households'].astype(float), lmbda = None)\n",
    "print(round(bc_params, 2))\n",
    "\n",
    "# Find the MLE for lambda (using sklearn's PowerTransformer)\n",
    "from sklearn.preprocessing import PowerTransformer\n",
    "power_transformer = PowerTransformer(method='box-cox', standardize=False)\n",
    "X_boxcox = power_transformer.fit_transform(X[['households']])\n",
    "print(round(power_transformer.lambdas_[0], 2))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eab5b771",
   "metadata": {},
   "source": [
    "### üö© Exercise 15 (EXTRA)\n",
    "\n",
    "- For the variable `households`, based on the `boxcox` transform shown above, do you think any of the values of $\\lambda$ may be useful? \n",
    "\n",
    "- Apply a similar code snippet to `median_house_value`. Would any values of $\\lambda$ be useful?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48e9fd8e",
   "metadata": {},
   "source": [
    "_Type your answer here!_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e7f0b09",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Code for your answer here!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae0bc68d",
   "metadata": {},
   "source": [
    "## Feature Combinations\n",
    "\n",
    "- Looking at the datas attributes we may also want to manually combine them into features that are either a more meaningful representation of the data or have better properties.\n",
    "\n",
    "- For example, we know **the number of rooms** in a district, but this may be more useful to combine with the **number of households** so that we have **a measure of rooms per household**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d71e961b",
   "metadata": {},
   "outputs": [],
   "source": [
    "rooms_per_household = X['total_rooms'] / X['households']\n",
    "rooms_per_household.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2260f36a",
   "metadata": {},
   "source": [
    "\n",
    "### üö© Exercise 16 (EXTRA)\n",
    "\n",
    "- Can you think of other combinations that may be useful?\n",
    "\n",
    "- Create a custom transformer that creates these new combinations of features using the `FunctionTransformer`.\n",
    "\n",
    "<br />\n",
    "<details><summary><b><u>Hint</b></u></summary>\n",
    "What about the following?\n",
    "   \n",
    "- `population_per_household`\n",
    "    \n",
    "- `bedrooms_per_room`\n",
    "    \n",
    "</details>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec68d54e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Code for your answer here!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a960a4e9",
   "metadata": {},
   "source": [
    "## Other feature types\n",
    "\n",
    "Feature engineering for other feature types beyond numerical categorical are also available in sklearn (e.g. for [text and images](https://scikit-learn.org/1.5/api/sklearn.feature_extraction.html)) and feature engine (e.g. for [Datetime](https://feature-engine.trainindata.com/en/1.8.x/user_guide/datetime/index.html) and for [time series](https://feature-engine.trainindata.com/en/1.8.x/user_guide/timeseries/index.html)). "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aabc1c5b",
   "metadata": {},
   "source": [
    "üèÅ **Now, is a good point to switch driver and navigator**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5bd9db96",
   "metadata": {},
   "source": [
    "## Combining into a Pipeline\n",
    "\n",
    "Now, that we are familar with transformers, we are finally ready to create our first model pipeline! \n",
    "\n",
    "Pipelines are very useful when we want to run data through our pipeline in the future; rather than having to copy and paste a load of code, we can just use our pipeline which combines all the steps. Later on the course, we will see this is important when we split our data into training, validation, and test sets, but this would also be required if you deploy your model in a \"live\" environment. In particular, pipelines  help prevent you from **data leakage**, i.e. when information from your testing data leaks into your training or model selection. **Data leakage** is a common reason why many ML models fail to generalize to real world data. Furthermore, when refining a model, pipelines makes it easier for us to add or remove steps of our pipeline to see what works and what doesn't.\n",
    "\n",
    "Its also worth examining what is meant by a **\"Pipeline\"**. A general definition is that it is just a sequence of data preparation operations that is ensured to be **reproducible**. Specifically, in sklearn,  `Pipeline` can contains a sequence of _transformer_ or _estimator_ classes, or, if we use an imbalanced-learn `Pipeline` instead, also _resamplers_. This week we have focused on _transformers_, but later on in the course we will learn about _estimators_ and _resamplers_. All three of these objects (_resamplers_, _transformers_, and _estimator_) all typically have a `.fit()` method. We have already seen examples of calling `.fit()` on _transformers_. The method works similarly on other classes and is used to\n",
    "- validate and interpret any parameters, \n",
    "- validate the input data, \n",
    "- estimate and store attributes from the parameters and provided data, \n",
    "- return the fitted estimator to facilitate method chaining in a pipeline. \n",
    "\n",
    "Along with other sample properties (e.g. `sample_weight`), the `.fit()` method usually takes two inputs:\n",
    "\n",
    "> - The input matrix (or design matrix) $\\mathbf{X}$. The size of $\\mathbf{X}$ is typically (n_samples, n_features), which means that samples are represented as rows and features are represented as columns.\n",
    ">\n",
    "> - The target values $\\mathbf{y}$ which are real numbers for regression tasks, or integers for classification (or any other discrete set of values). For unsupervised learning tasks, $\\mathbf{y}$ does not need to be specified. \n",
    ">\n",
    "> https://scikit-learn.org/stable/getting_started.html\n",
    "\n",
    "Other methods available for these objects other than `.fit()` will depend on what they are, e.g. `.transform()` for transformers, so we will learn about the methods for others objects later in the course. \n",
    "\n",
    "This week, our focus is combining different feature engineering steps together to make different model pipelines.\n",
    "\n",
    "- Remember we want to create a pipeline that treats the **numerical** and **categorical** attributes differently. \n",
    "- We also need to supply the pipeline with an _estimator_ (i.e. model). For now, let's use a linear regression model, which we will learn in more details in week 4."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b7744d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "numcols = features[:-1]\n",
    "catcols = [features[-1]]\n",
    "\n",
    "num_pre = Pipeline([\n",
    "    (\"num_impute\", SimpleImputer(strategy=\"median\")),\n",
    "    (\"num_scale\", StandardScaler())])\n",
    "\n",
    "cat_pre = Pipeline([\n",
    "    (\"cat_encode\", OneHotEncoder(drop='first'))])\n",
    "\n",
    "reg_pipe_1 = Pipeline([\n",
    "    (\"pre_processing\", ColumnTransformer([(\"num_pre\", num_pre, numcols),\n",
    "                                          (\"cat_pre\", cat_pre, catcols)], \n",
    "                                          verbose_feature_names_out=False)),\n",
    "    (\"model\", LinearRegression())\n",
    "])\n",
    "\n",
    "# Alternative and equivalent model avoiding nested pipelines\n",
    "# reg_pipe_1 = Pipeline([\n",
    "#     (\"impute\", ColumnTransformer([(\"num_imp\", SimpleImputer(strategy=\"median\"), numcols),\n",
    "#                                           (\"cat_imp\", SimpleImputer(strategy=\"constant\"), catcols)])),\n",
    "#     (\"transform\", ColumnTransformer([(\"num_trns\", StandardScaler(), numcols),\n",
    "#                                           (\"cat_trns\", OneHotEncoder(drop='first'), catcols)])),                                     \n",
    "#     (\"model\", LinearRegression())\n",
    "# ])\n",
    "\n",
    "display(reg_pipe_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8466a426",
   "metadata": {},
   "outputs": [],
   "source": [
    "reg_pipe_1.fit(X,y)\n",
    "# Print the R squared (ranges 0 to 1, with higher values better)\n",
    "print(round(reg_pipe_1.score(X, y), 3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a82293d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print the coeffcients\n",
    "coef_df = pd.DataFrame({'coef': reg_pipe_1['model'].coef_}, \n",
    "             index = reg_pipe_1['pre_processing'].get_feature_names_out())\n",
    "display(coef_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89b2ffa6",
   "metadata": {},
   "source": [
    "Let's try some other combinations of the pre-processing and feature engineering steps that we have learned about this week."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2d0a45e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reg Pipe 2 \n",
    "\n",
    "# Define column indices\n",
    "numcols = ['longitude', 'latitude', 'housing_median_age', 'median_income']\n",
    "countcols = ['total_rooms', 'total_bedrooms', 'population', 'households']\n",
    "\n",
    "# Reg Pipe 2 \n",
    "num_pre = Pipeline([\n",
    "    (\"num_scale\", StandardScaler())])\n",
    "\n",
    "count_pre = Pipeline([\n",
    "    (\"count_impute\", SimpleImputer(strategy=\"median\")),\n",
    "    (\"count_transform\", PowerTransformer(method='box-cox', standardize=True))])\n",
    "\n",
    "cat_pre = Pipeline([\n",
    "    (\"cat_encode\", OneHotEncoder(drop='first'))])\n",
    "\n",
    "# Overall ML pipeline inlcuding all \n",
    "reg_pipe_2 = Pipeline([\n",
    "    (\"pre_processing\", ColumnTransformer([\n",
    "        (\"num_pre\", num_pre, numcols), \n",
    "        (\"count_pre\", count_pre, countcols),\n",
    "        (\"cat_pre\", cat_pre, catcols)], verbose_feature_names_out=False)), \n",
    "    (\"model\", LinearRegression())\n",
    "])\n",
    "\n",
    "\n",
    "display(reg_pipe_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8b44b71",
   "metadata": {},
   "outputs": [],
   "source": [
    "reg_pipe_2.fit(X,y)\n",
    "# Print the R squared (ranges 0 to 1, with higher values better)\n",
    "print(round(reg_pipe_2.score(X, y), 3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c977962f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print the coeffcients\n",
    "coef_df = pd.DataFrame({'coef': reg_pipe_2['model'].coef_}, \n",
    "             index = reg_pipe_2['pre_processing'].get_feature_names_out())\n",
    "display(coef_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e32681c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reg Pipe 3\n",
    "from feature_engine.transformation import LogTransformer\n",
    "from sklearn.compose import TransformedTargetRegressor\n",
    "\n",
    "numcols = ['longitude', 'latitude', 'housing_median_age']\n",
    "skewcols = ['total_rooms', 'total_bedrooms', 'population', 'households', 'median_income']\n",
    "\n",
    "num_pre = Pipeline([\n",
    "    (\"num_scale\", StandardScaler())])\n",
    "\n",
    "skew_pre = Pipeline([\n",
    "    (\"skew_impute\", SimpleImputer(strategy=\"median\")),\n",
    "    (\"skew_transform\", LogTransformer()),\n",
    "    (\"skew_scale\", StandardScaler())])\n",
    "\n",
    "cat_pre = Pipeline([\n",
    "    (\"cat_encode\", OneHotEncoder(drop='first'))])\n",
    "\n",
    "# Overall ML pipeline inlcuding all \n",
    "reg_pipe_3 = Pipeline([\n",
    "    (\"pre_processing\", ColumnTransformer([\n",
    "        (\"num_pre\", num_pre, numcols), \n",
    "        (\"skew_pre\", skew_pre, skewcols), \n",
    "        (\"cat_pre\", cat_pre, catcols)], verbose_feature_names_out=False)), \n",
    "    (\"model\", LinearRegression())\n",
    "])\n",
    "# Transform also the target variable\n",
    "tt_reg_pipe_3 =TransformedTargetRegressor(regressor=reg_pipe_3,\n",
    "                                  transformer=LogTransformer())\n",
    "\n",
    "display(tt_reg_pipe_3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a95e7ddc",
   "metadata": {},
   "outputs": [],
   "source": [
    "tt_reg_pipe_3.fit(X,y)\n",
    "# Print the R squared (ranges 0 to 1, with higher values better)\n",
    "print(round(tt_reg_pipe_3.score(X, y), 3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3e65c25",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print the coeffcients\n",
    "# Note: get_feature_names_out() does not work for LogTransformer\n",
    "reg3_features = np.concatenate([tt_reg_pipe_3.regressor_['pre_processing']['num_pre'].get_feature_names_out(),\n",
    "               tt_reg_pipe_3.regressor_['pre_processing']['skew_pre'].feature_names_in_,\n",
    "               tt_reg_pipe_3.regressor_['pre_processing']['cat_pre'].get_feature_names_out()]\n",
    ")\n",
    "\n",
    "coef_df = pd.DataFrame({'coef': tt_reg_pipe_3.regressor_['model'].coef_}, \n",
    "             index = reg3_features) \n",
    "display(coef_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49a8dd4a",
   "metadata": {},
   "source": [
    "### üö© Exercise 17 (CORE)\n",
    "\n",
    "Explain in words what are the differences in pre-processing and/or feature engineering steps used across the three model pipelines above. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c430de7",
   "metadata": {},
   "source": [
    "_Type your answer here!_"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2daf4138",
   "metadata": {},
   "source": [
    "### üö© Exercise 17 (EXTRA)\n",
    "\n",
    "Try to create your own pipeline by modifying at least one of the pre-processing and feature engineering steps above. What have you decided to change and why?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04ab501d",
   "metadata": {},
   "source": [
    "# Summary <a id='sum'></a>\n",
    "\n",
    "This week we covered a lot of ground!\n",
    "\n",
    "- We've looked at some methods for pre-processing our data, cleaning and preparing it, as well as how to engineer some features and combine these steps into a reproducible pipeline.\n",
    "\n",
    "- This is by **no means a complete collection of all the methods available** as covering more would go beyond the scope of this course (for those interested in learning more, have a look though the given companion readings). \n",
    "\n",
    "- For example, we did not touch on handling text and dates/time much. These topics are quite complex and have enough materials to cover their own courses."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a54e7817",
   "metadata": {},
   "source": [
    "# Competing the Worksheet\n",
    "\n",
    "At this point you have hopefully been able to complete all the CORE exercises and attempted the EXTRA ones. Now \n",
    "is a good time to check the reproducibility of this document by restarting the notebook's\n",
    "kernel and rerunning all cells in order.\n",
    "\n",
    "Before generating the PDF, please **change 'Student 1' and 'Student 2' at the top of the notebook to include your name(s)**. \n",
    "\n",
    "Once that is done and you are happy with everything, you can then run the following cell \n",
    "to generate your PDF."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "937dba8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "!jupyter nbconvert --to pdf mlp_week01.ipynb "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7218599",
   "metadata": {},
   "source": [
    "Once generated, please submit this PDF on Learn page by 16:00 PM on the Friday of the week the workshop was given. Note that:\n",
    "\n",
    ">- You don't need to finish everything, but you should have had a substantial attempt at the bulk of the material, particularly the CORE tasks. \n",
    ">- If you are having trouble generating the pdf, please ask a tutor or post on piazza. \n",
    ">- As a back option, if you are having errors in converting to pdf, then a quick solution is to export to html and then convert to pdf in your browser."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  },
  "title": "MLPy Workshop 1"
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
